{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "from nasbench import api\n",
    "import numpy as np\n",
    "from functools import cmp_to_key\n",
    "\n",
    "nasbench = api.NASBench('./data/nasbench_full.tfrecord')\n",
    "print(len(nasbench.fixed_statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val and Test accuracy on NAS-Bench-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_c10  = {}\n",
    "acc_c10  = {}\n",
    "for i in tqdm(range(len(nasbench.fixed_statistics))):\n",
    "    hashvalue = list(nasbench.fixed_statistics.keys())[i]\n",
    "    # val acc on cifar10\n",
    "    n = np.random.randint(3)\n",
    "    val_acc_c10[hashvalue] = nasbench.computed_statistics[hashvalue][12][n]['final_validation_accuracy']\n",
    "    # test acc on cifar10\n",
    "    acc_c10[hashvalue] = ( nasbench.computed_statistics[hashvalue][108][0]['final_test_accuracy'] + \n",
    "                           nasbench.computed_statistics[hashvalue][108][1]['final_test_accuracy'] + \n",
    "                           nasbench.computed_statistics[hashvalue][108][2]['final_test_accuracy'] ) / 3\n",
    "\n",
    "val_accs = {'cifar10': val_acc_c10}\n",
    "accs = {'cifar10': acc_c10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of (nb1*.p path, dataset) and names of all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pickle\n",
    "def search_file(pattern, search_path):\n",
    "    for path in search_path.split(os.pathsep):\n",
    "        for match in glob.glob(os.path.join(path, pattern)):\n",
    "            yield match\n",
    "\n",
    "path_list = list(search_file('nb1*.p', './'))\n",
    "p_list = [(x, 'ImageNet16-120' if 'im120' in x else 'cifar100' if 'cf100' in x else 'cifar10', 3 if 'im120' in x else 2 if 'cf100' in x else 1) for x in path_list]\n",
    "p_list = sorted(p_list,key=lambda x: x[2])\n",
    "print(p_list)\n",
    "metric_names = ['val_acc'] + list(pickle.load(open(p_list[0][0],'rb'))['logmeasures'].keys())\n",
    "print(metric_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vote setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orignal_vote_metric_names = ['snip', 'synflow', 'jacob_cor']\n",
    "orignal_vote_signs = [-1, 1, -1]\n",
    "our_vote_1_metric_names = ['synflow', 'jacob_cor', 'act_grad_cor_weighted']\n",
    "our_vote_1_signs = [1, -1, 1]\n",
    "vote_metric_names = [orignal_vote_metric_names, our_vote_1_metric_names]\n",
    "vote_signs        = [orignal_vote_signs, our_vote_1_signs]\n",
    "metric_names.extend(['ori_vote', 'our_vote'])\n",
    "print(metric_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft vote and Hard vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_vote(vote_metric_names, vote_signs, metrics, normlize='zscore'):\n",
    "    vote_metrics = []\n",
    "    for i, k in enumerate(vote_metric_names):\n",
    "        metric_nparray = np.array(vote_signs[i] * metrics[k])\n",
    "        if normlize == 'minmax':\n",
    "            metric_nparray = (metric_nparray - np.ma.masked_invalid(metric_nparray).min()) / (np.ma.masked_invalid(metric_nparray).max() - np.ma.masked_invalid(metric_nparray).min())\n",
    "        elif normlize == 'zscore':\n",
    "            metric_nparray = (metric_nparray - np.ma.masked_invalid(metric_nparray).mean()) / np.ma.masked_invalid(metric_nparray).std()\n",
    "        else:\n",
    "            raise ValueError('No normlize {}.'.format(normlize))\n",
    "        vote_metrics.append(metric_nparray)\n",
    "    return (sum(vote_metrics)/len(vote_metrics)).tolist()\n",
    "\n",
    "def hard_vote(vote_metric_names, vote_signs, metrics):\n",
    "    num_archs = len(metrics[list(metrics.keys())[0]])\n",
    "    archs_idx = list(range(num_archs))\n",
    "    \n",
    "    def cmp(idx1, idx2):\n",
    "        ret = []\n",
    "        for i, k in enumerate(vote_metric_names):\n",
    "            ret.append(vote_signs[i] * (metrics[k][idx1] - metrics[k][idx2]))\n",
    "        ret = np.array(ret)\n",
    "        if sum(ret<0)>len(vote_metric_names)/2: return -1\n",
    "        elif sum(ret>0)>len(vote_metric_names)/2: return 1\n",
    "        else: return 0\n",
    "    \n",
    "    sorted_archs_idx = sorted(archs_idx, key=cmp_to_key(cmp))\n",
    "    archs_idx_ranking = []\n",
    "    for x in archs_idx:\n",
    "        archs_idx_ranking.append(sorted_archs_idx.index(x))\n",
    "    return archs_idx_ranking\n",
    "\n",
    "def vote(vote_metric_names, vote_signs, metrics, mode):\n",
    "    if 'hard' in mode:\n",
    "        return hard_vote(vote_metric_names, vote_signs, metrics)\n",
    "    elif 'soft' in mode:\n",
    "        if 'zscore' in mode:\n",
    "            return soft_vote(vote_metric_names, vote_signs, metrics, normlize='zscore')\n",
    "        elif 'minmax' in mode:\n",
    "            return soft_vote(vote_metric_names, vote_signs, metrics, normlize='minmax')\n",
    "        else:\n",
    "            raise ValueError('No {:} mode'.format(mode))\n",
    "    else:\n",
    "        raise ValueError('No {:} mode'.format(mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kendall τ distance of zero-cost proxies on NAS-Bench-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=None\n",
    "hl=['Dataset']\n",
    "hl.extend(metric_names)\n",
    "t = PrettyTable(hl)\n",
    "\n",
    "for fname, rname, _ in p_list:\n",
    "    runs=[]\n",
    "    f = open(fname,'rb')\n",
    "    while(1):\n",
    "        try:\n",
    "            runs.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break\n",
    "    f.close()\n",
    "    print(fname, len(runs))\n",
    "\n",
    "    metrics={}\n",
    "    for k in metric_names:\n",
    "        metrics[k] = []\n",
    "    acc = []\n",
    "\n",
    "    for r in runs:\n",
    "        for k,v in r['logmeasures'].items():\n",
    "            if k in metrics:\n",
    "                metrics[k].append(v)\n",
    "        metrics['val_acc'].append(val_accs[rname][r['hash']])\n",
    "        acc.append(accs[rname][r['hash']])\n",
    "\n",
    "    for i, (vote_metric_name, vote_sign) in enumerate(reversed(list(zip(vote_metric_names, vote_signs)))):\n",
    "        metrics[metric_names[-(i+1)]] = vote(vote_metric_name, vote_sign, metrics, mode='hard')\n",
    "\n",
    "    res = []\n",
    "    for k in hl:\n",
    "        if k=='Dataset':\n",
    "            continue\n",
    "        v = metrics[k]\n",
    "        cr = stats.kendalltau(acc, v, nan_policy='omit')[0]\n",
    "        res.append(round(cr,3))\n",
    "\n",
    "    t.add_row([rname]+res)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman ρ of zero-cost proxies on NAS-Bench-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=None\n",
    "hl=['Dataset']\n",
    "hl.extend(metric_names)\n",
    "t = PrettyTable(hl)\n",
    "\n",
    "for fname, rname, _ in p_list:\n",
    "    runs=[]\n",
    "    f = open(fname,'rb')\n",
    "    while(1):\n",
    "        try:\n",
    "            runs.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break\n",
    "    f.close()\n",
    "    print(fname, len(runs))\n",
    "\n",
    "    metrics={}\n",
    "    for k in metric_names:\n",
    "        metrics[k] = []\n",
    "    acc = []\n",
    "\n",
    "    for r in runs:\n",
    "        for k,v in r['logmeasures'].items():\n",
    "            if k in metrics:\n",
    "                metrics[k].append(v)\n",
    "        metrics['val_acc'].append(val_accs[rname][r['hash']])\n",
    "        acc.append(accs[rname][r['hash']])\n",
    "\n",
    "    for i, (vote_metric_name, vote_sign) in enumerate(reversed(list(zip(vote_metric_names, vote_signs)))):\n",
    "        metrics[metric_names[-(i+1)]] = vote(vote_metric_name, vote_sign, metrics, mode='hard')\n",
    "\n",
    "    res = []\n",
    "    for k in hl:\n",
    "        if k=='Dataset':\n",
    "            continue\n",
    "        v = metrics[k]\n",
    "        cr = stats.spearmanr(acc, v, nan_policy='omit').correlation\n",
    "        res.append(round(cr,3))\n",
    "\n",
    "    t.add_row([rname]+res)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origin paper result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+---------+-----------+-------+-------+--------+---------+-----------+\n",
    "| Dataset | grad_norm |  snip | grasp | fisher | synflow | jacob_cov |\n",
    "+---------+-----------+-------+-------+--------+---------+-----------+\n",
    "| CIFAR10 |   0.198   | 0.164 | 0.448 | 0.257  |  0.372  |   0.378   |\n",
    "+---------+-----------+-------+-------+--------+---------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
